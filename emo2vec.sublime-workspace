{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"emole",
				"emolex_class_distr\tstatement"
			],
			[
				"Rms",
				"RMSprop\tclass"
			],
			[
				"Twit",
				"TwitterError\tmodule"
			],
			[
				"line",
				"line_idx\tstatement"
			],
			[
				"word",
				"word2idx\tstatement"
			]
		]
	},
	"buffers":
	[
		{
			"contents": "from keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization, Activation\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.regularizers import l2\nimport numpy as np\nfrom keras import optimizers\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score\nimport hashtag_corpus\nnp.random.seed(4)\n\nPRETRAINED_MODEL = 'resources/emotion_specific/bilstm_300d.txt'\nLEXICON_PATH = 'resources/data/emolex.txt'\nBATCH_SIZE = 128\nEMBEDDING_DIM = 300\nEPOCHS = 50\nTRAIN_SIZE = 0.8\nVALID_SIZE = 0.0\nNUM_EMOTIONS = 6\n\n\ndef read_emo_lemma(aline):\n    \"\"\"\n    Splits a line into lemma l, emotion e, and l(e).\n    l(e) := 1 if lemma l has emotion e according to the lexicon\n    l(e) := 0 otherwise\n    \"\"\"\n    split = aline.split()\n    return split[0], split[1], int(split[2])\n\n\nx_train, y_train, x_val, y_val, x_test, y_test = hashtag_corpus.split(TRAIN_SIZE, VALID_SIZE)\nword_to_index = hashtag_corpus.word_index()\nV = len(word_to_index)\n\nprint('Index word vectors...')\nembeddings_index = {}\nwith open(PRETRAINED_MODEL, 'r', encoding='UTF-8') as f:\n    next(f)  # skip header\n    for line in f:\n        values = line.split()\n        if len(values) != EMBEDDING_DIM + 1:  # probably an error occurred during tokenization\n            continue\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\nprint('Found %s word vectors.' % len(embeddings_index))\n\nlexicon = dict()\nwith open(LEXICON_PATH, 'r') as f:\n    emo_idx = 0  # anger: 0, disgust: 1, fear: 2, joy: 3, sadness: 4, surprise: 5\n    for l in f:\n        lemma, emotion, has_emotion = read_emo_lemma(l)\n        if emotion == 'anger':  # i.e. if lemma not in lexicon.keys()\n            lexicon[lemma] = np.empty(shape=(NUM_EMOTIONS,), dtype='float32')\n        if emotion in ['positive', 'negative', 'anticipation', 'trust']:\n            continue\n        lexicon[lemma][emo_idx] = has_emotion\n        if emo_idx < NUM_EMOTIONS - 1:\n            emo_idx += 1\n        else:\n            # reset index - next line contains a new lemma\n            emo_idx = 0\n\nexp_lexicon = {}\nwith open('y_1_2000-1000-5.txt', 'r', encoding='utf-8') as f:\n    for line in f:\n        split = line.split()\n        if len(split) != NUM_EMOTIONS + 1:  # probably an error occurred during tokenization\n            print(line)\n            continue\n        word = split[0]\n        probs = np.asarray(split[1:], dtype='float32')\n        exp_lexicon[word] = probs\n\nemolex_class_distr = np.array([1247, 1058, 1476, 689, 1191, 534])\nemolex_class_distr = emolex_class_distr / np.sum(emolex_class_distr)\n\nfor word, coef in embeddings_index.items():\n    try:\n        # lexicon[word][lexicon[word] == 0] = 1e-10\n        embeddings_index[word] = np.append(coef, exp_lexicon[word])\n    except KeyError:\n        embeddings_index[word] = np.append(coef, emolex_class_distr)\n\n\nprint('Prepare embedding matrix...')\nembedding_matrix = np.random.normal(size=(V + 1, EMBEDDING_DIM+NUM_EMOTIONS))\nfor word, i in word_to_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i - 1] = embedding_vector\n\nprint('Build model...')\n\nmodel = Sequential()\nmodel.add(Embedding(V + 1,\n                    EMBEDDING_DIM+NUM_EMOTIONS,\n                    weights=[embedding_matrix],\n                    input_length=hashtag_corpus.max_sequence_len(),\n                    trainable=True))\nmodel.add(BatchNormalization())\nmodel.add(Bidirectional(\n    (LSTM(128, dropout=0.1, recurrent_dropout=0.2, recurrent_activation='sigmoid', recurrent_regularizer=l2()))))\nmodel.add(BatchNormalization())\nmodel.add(Dense(NUM_EMOTIONS, activation='softmax'))\n\nadagrad = optimizers.Adagrad(lr=0.005, epsilon=1e-08, decay=1e-5)\n\nmodel.compile(loss='categorical_crossentropy', optimizer=adagrad)\n\nprint(model.summary())\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    shuffle=True,\n                    validation_data=(x_val, y_val))\n\nprint(history.history)\n\npreds = model.predict_classes(x_test, verbose=True)\nY = [np.argmax(x) for x in y_test]\n\nprint(classification_report(Y, preds))\n\nprint(precision_score(Y, preds, average='micro'),\n      recall_score(Y, preds, average='micro'),\n      f1_score(Y, preds, average='micro'))\n",
			"file": "new/classifier_exp_lex.py",
			"file_size": 4523,
			"file_write_time": 131436317500000000,
			"settings":
			{
				"buffer_size": 4523,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "import numpy as np\nimport tensorflow as tf\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import normalize\nfrom math import floor, pi\nimport sys\nnp.random.seed(13)\n\nif len(sys.argv) != 4:\n    sys.exit('Usage: batch_size, num_batches, num_epochs')\n\n# Lazy way to avoid overwriting old output files\nf = open('log_5000-1000-3.txt', 'x')\nf.close()\n\nclass Model:\n    def __init__(self, n_labeled, n_unlabeled, input_dims, n_classes):\n        self._t_uu = t_uu = tf.placeholder(tf.float32, shape=[n_unlabeled, n_unlabeled, input_dims])\n        self._t_ul = t_ul = tf.placeholder(tf.float32, shape=[n_unlabeled, n_labeled, input_dims])\n        self._y_l = y_l = tf.placeholder(tf.float32, shape=[n_labeled, n_classes])\n\n        w_init = tf.random_uniform(shape=[input_dims,], minval=0.5, maxval=5)\n        self._w = w = tf.get_variable(\"w\", dtype=tf.float32, initializer=w_init)\n\n\n        b_init = tf.random_uniform(shape=[], minval=-1, maxval=1)\n        self._b = b = tf.get_variable(\"b\", dtype=tf.float32, initializer=b_init)\n\n        tuu = tf.sigmoid(tf.reduce_sum(w * t_uu, axis=2) + b)\n        tul = tf.sigmoid(tf.reduce_sum(w * t_ul, axis=2) + b)\n        # tuu = tf.Print(tuu, [tuu], 'tuu', summarize=30)\n        # tul = tf.Print(tul, [tul], 'tul', summarize=30)\n\n        uniform_init = tf.constant_initializer(1 / n_unlabeled+n_labeled, dtype=tf.float32)\n        u1 = tf.get_variable(\"uniform1\",\n                             dtype=tf.float32,\n                             shape=[n_unlabeled, n_unlabeled],\n                             trainable=False,\n                             initializer=uniform_init)\n        u2 = tf.get_variable(\"uniform2\",\n                             dtype=tf.float32,\n                             shape=[n_unlabeled, n_labeled],\n                             initializer=uniform_init)\n\n        self._epsilon = epsilon = tf.get_variable(\"epsilon\",\n                                                  dtype=tf.float32,\n                                                  shape=[],\n                                                  trainable=True,\n                                                  initializer=tf.constant_initializer(0.5e-4, dtype=tf.float32))\n        # epsilon = tf.Print(epsilon, [epsilon], 'Epsilon: ')\n        tuu = epsilon * u1 + (1 - epsilon) * tuu\n        tul = epsilon * u2 + (1 - epsilon) * tul\n\n        # column normalization\n        tuu_col_norms = tf.norm(tuu, ord=1, axis=0)\n        tul_col_norms = tf.norm(tul, ord=1, axis=0)\n        tuu /= tuu_col_norms\n        tul /= tul_col_norms\n\n        # row normalization\n        tuu_row_norms = tf.norm(tuu, ord=1, axis=1)\n        tul_row_norms = tf.norm(tul, ord=1, axis=1)\n        tuu /= tf.reshape(tuu_row_norms, [n_unlabeled, 1])\n        tul /= tf.reshape(tul_row_norms, [n_unlabeled, 1])\n\n        I = tf.eye(n_unlabeled, dtype=tf.float32)\n        inv = tf.matrix_solve_ls((I - tuu), I, l2_regularizer=0.01)\n\n        y_u = tf.matmul(tf.matmul(inv, tul), y_l)\n\n        y = tf.concat([y_u, y_l], 0)\n        self._y = y = tf.clip_by_value(y, 1e-15, float(\"inf\"))\n\n        self._entropy = entropy = - tf.reduce_sum(y * tf.log(y))\n        self._train_op = tf.train.AdamOptimizer(0.001).minimize(entropy)\n\n\n    @property\n    def t_uu(self):\n        return self._t_uu\n\n    @property\n    def t_ul(self):\n        return self._t_ul\n\n    @property\n    def y_l(self):\n        return self._y_l\n\n    @property\n    def y(self):\n        return self._y\n\n    @property\n    def train_op(self):\n        return self._train_op\n\n    @property\n    def entropy(self):\n        return self._entropy\n\n    @property\n    def w(self):\n        return self._w\n\n    @property\n    def b(self):\n        return self._b\n\n\ndef read_emo_lemma(aline):\n    \"\"\"\n    Splits a line into lemma l, emotion e, and l(e).\n    l(e) := 1 if lemma l has emotion e according to the lexicon\n    l(e) := 0 otherwise\n    \"\"\"\n    split = aline.split()\n    return split[0], split[1], int(split[2])\n\n\nn_emotions = 6\nembed_dim = 300\n\n_embeddings = []\nword2idx = {}\nline_idx = 0\nwith open('resources/emotion_specific/bilstm_300d.txt', 'r', encoding='UTF-8') as f:\n    next(f)  # skip header\n\n    for line in f:\n        values = line.split()\n\n        # probably an error occurred during tokenization\n        if len(values) != embed_dim + 1:\n            continue\n\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n\n        # skip all-zeros vectors\n        if not coefs.any():\n            continue\n\n        # only one vector for each word\n        try:\n            word2idx[word]\n        except:\n            _embeddings.append(coefs)\n            word2idx[word] = line_idx\n            line_idx += 1\n\nn = line_idx\nprint('Found', n, 'word vectors.')\n\nembeddings = np.asarray(_embeddings, dtype='float32')\nembeddings = normalize(embeddings, axis=1, norm='l2', copy=False)\n\n\nprint('Build distance matrix.')\ny_l = np.empty(shape=(14182, n_emotions), dtype='float32')\n\nlexeme2index = dict()\nwith open('resources/data/emolex.txt', 'r') as f:\n    emo_idx = 0  # anger: 0, disgust: 1, fear: 2, joy: 3, sadness: 4, surprise: 6\n    i = 0\n    for l in f:\n        lemma, emotion, has_emotion = read_emo_lemma(l)\n        if emotion == 'anger':  # i.e. if lemma not in lexicon.keys()\n            lexeme2index[lemma] = i\n        if emotion in ['positive', 'negative', 'anticipation', 'trust']:\n            continue\n        y_l[i][emo_idx] = has_emotion\n        if emo_idx < n_emotions - 1:\n            emo_idx += 1\n        else:\n            # reset index - next line contains a new lemma\n            emo_idx = 0\n            i += 1\n\nprint('Initialize label distribution matrix.')\ny = np.random.random((n, n_emotions))\n\nlabeled_indices = []\nfor word, idx in lexeme2index.items():\n    try:\n        # if word in corpus\n        idx_T = word2idx[word]  # get index of word in T\n        y[idx_T] = y_l[idx]  # set values of labeled word\n        labeled_indices.append(idx_T)\n    except KeyError:\n        continue\n\n# turn multi-labels into prob distribution\ny = normalize(y, axis=1, norm='l1', copy=False)\n\nl = labeled_indices\nu = np.setdiff1d(np.asarray(list(word2idx.values()), dtype='int32'), l)\n\ntot_batch_size = int(sys.argv[1])\nn_batches = int(sys.argv[2])\nn_epochs = int(sys.argv[3])\nl_batch_size = int(tot_batch_size * (5869 / n))\nu_batch_size = tot_batch_size - l_batch_size\n\nwith open('log_5000-1000-3.txt', 'w') as f:\n    print('Batch size:', tot_batch_size, file=f)\n    print('Number of batchs:', n_batches, file=f)\n    print('Number of epochs:', n_epochs, \"\\n\", file=f)\n\nprint('Tensorflow.')\nsess = tf.Session()\n\nwith tf.variable_scope(\"model\", reuse=None) as scope:\n    model = Model(l_batch_size, u_batch_size, embed_dim, n_emotions)\n\nsess.run(tf.global_variables_initializer())\n\nf = open('log_5000-1000-3.txt', 'a')\n\nfor batch in range(n_batches):\n    print('Batch', batch)\n    rand_u = np.random.choice(u, size=u_batch_size, replace=False)\n    rand_l = np.random.choice(l, size=l_batch_size, replace=False)\n\n    yl_batch = np.asarray(y[rand_l])\n    tuu_batch = np.empty((u_batch_size, u_batch_size, embed_dim), dtype='float32')\n    tul_batch = np.empty((u_batch_size, l_batch_size, embed_dim), dtype='float32')\n\n    for (j, j_) in enumerate(rand_u):\n        for (k, k_) in enumerate(rand_u):\n            tuu_batch[j, k] = embeddings[j_] * embeddings[k_]\n        for (k, k_) in enumerate(rand_l):\n            tul_batch[j, k] = embeddings[j_] * embeddings[k_]\n\n    for epoch in range(n_epochs):\n        h, _, w, b = sess.run([model.entropy, model.train_op, model.w, model.b],\n                              {model.t_uu: tuu_batch, model.t_ul: tul_batch, model.y_l: yl_batch})\n\n        print(epoch, ': ', h, sep='')\n\n        if batch == n_batches - 1 and epoch == n_epochs - 1:\n            print('\\n\\nEntropy:', h, file=f)\n            print('w:', w, '\\n', file=f)\n            print('b', b, '\\n', file=f)\n            print('Output saved to emo2vec/log_5000-1000-3.txt')\n\nf.close()\nsess.close()\n",
			"file": "new/tensor_lp_300.py",
			"file_size": 7951,
			"file_write_time": 131438320610000000,
			"settings":
			{
				"buffer_size": 7951,
				"line_ending": "Unix"
			}
		},
		{
			"file": "new/crossval_1.py",
			"settings":
			{
				"buffer_size": 6833,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "lp_300.py",
			"settings":
			{
				"buffer_size": 9102,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "crossval_300.py",
			"settings":
			{
				"buffer_size": 10272,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensor_lp_300.py",
			"settings":
			{
				"buffer_size": 7951,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensor_lp_1.py",
			"settings":
			{
				"buffer_size": 6212,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "import numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import normalize\nfrom math import acos, pi\nimport sys\nnp.random.seed(13)\n\nif len(sys.argv) == 2:\n    STOP = int(sys.argv[1])\nelif len(sys.argv) == 1:\n    STOP = 50000  # i.e. no number of words limit\nelse:\n    sys.exit('Usage: lp_1.py [word limit]')\n\nclass Model:\n    def __init__(self, n_labeled, n_unlabeled, n_classes):\n        self._t_uu = t_uu = tf.placeholder(tf.float32, shape=[n_unlabeled, n_unlabeled])\n        self._t_ul = t_ul = tf.placeholder(tf.float32, shape=[n_unlabeled, n_labeled])\n        self._y_l = y_l = tf.placeholder(tf.float32, shape=[n_labeled, n_classes])\n\n        self._w = w = tf.placeholder(tf.float32, shape=[])\n        self._b = b = tf.placeholder(tf.float32, shape=[])\n\n        tuu = tf.sigmoid(w * t_uu + b)\n        tul = tf.sigmoid(w * t_ul + b)\n\n        # column normalization\n        tuu_col_norms = tf.norm(tuu, ord=1, axis=0)\n        tul_col_norms = tf.norm(tul, ord=1, axis=0)\n        tuu /= tuu_col_norms\n        tul /= tul_col_norms\n\n        # row normalization\n        tuu_row_norms = tf.norm(tuu, ord=1, axis=1)\n        tul_row_norms = tf.norm(tul, ord=1, axis=1)\n        tuu /= tf.reshape(tuu_row_norms, [n_unlabeled, 1])\n        tul /= tf.reshape(tul_row_norms, [n_unlabeled, 1])\n\n        I = tf.eye(n_unlabeled, dtype=tf.float32)\n        inv = tf.matrix_solve_ls((I - tuu), I, l2_regularizer=0.01)\n\n        y_u = tf.matmul(tf.matmul(inv, tul), y_l)\n\n        y = tf.concat([y_u, y_l], 0)\n        self._y = y = tf.clip_by_value(y, 1e-15, float(\"inf\"))\n\n\n    @property\n    def t_uu(self):\n        return self._t_uu\n\n    @property\n    def t_ul(self):\n        return self._t_ul\n\n    @property\n    def y_l(self):\n        return self._y_l\n\n    @property\n    def y(self):\n        return self._y\n\n    @property\n    def w(self):\n        return self._w\n\n    @property\n    def b(self):\n        return self._b\n\n\ndef read_emo_lemma(aline):\n    \"\"\"\n    Splits a line into lemma l, emotion e, and l(e).\n    l(e) := 1 if lemma l has emotion e according to the lexicon\n    l(e) := 0 otherwise\n    \"\"\"\n    split = aline.split()\n    return split[0], split[1], int(split[2])\n\n\n\nNUM_EMOTIONS = 6\nNDIMS = 300\n\n_embeddings = []\nword2idx = {}\nline_idx = 0\nwith open('resources/emotion_specific/bilstm_300d.txt', 'r', encoding='UTF-8') as f:\n    next(f)  # skip header\n\n    for line in f:\n        if line_idx >= STOP:\n            break\n\n        values = line.split()\n\n        # probably an error occurred during tokenization\n        if len(values) != NDIMS + 1:\n            continue\n\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n\n        # skip all-zeros vectors\n        if not coefs.any():\n            continue\n\n        # only one vector for each word\n        try:\n            word2idx[word]\n        except:\n            _embeddings.append(coefs)\n            word2idx[word] = line_idx\n            line_idx += 1\n\nn = line_idx\nprint('Found', n, 'word vectors.')\n\nembeddings = np.asarray(_embeddings, dtype='float32')\nembeddings = normalize(embeddings, axis=1, norm='l2', copy=False)\n\nprint('Build distance matrix.')\nt = np.empty((n, n), dtype='float32')\n\nlog_count = 0\nfor j in word2idx.values():\n    for k in word2idx.values():\n        t[j, k] = embeddings[j] @ embeddings[k]\n\n    log_count += 1\n    if log_count % 1000 == 0:\n        print(log_count, \"/\", n, sep=\"\")\n\ny_l = np.empty(shape=(14182, NUM_EMOTIONS), dtype='float32')\nlexeme2index = dict()\nwith open('resources/data/emolex.txt', 'r') as f:\n    emo_idx = 0  # anger: 0, disgust: 1, fear: 2, joy: 3, sadness: 4, surprise: 6\n    i = 0\n    for l in f:\n        lemma, emotion, has_emotion = read_emo_lemma(l)\n        if emotion == 'anger':  # i.e. if lemma not in lexicon.keys()\n            lexeme2index[lemma] = i\n        if emotion in ['positive', 'negative', 'anticipation', 'trust']:\n            continue\n        y_l[i][emo_idx] = has_emotion\n        if emo_idx < NUM_EMOTIONS - 1:\n            emo_idx += 1\n        else:\n            # reset index - next line contains a new lemma\n            emo_idx = 0\n            i += 1\n\nprint('Initialize label distribution matrix.')\ny = np.random.random((n, NUM_EMOTIONS))\n\nlabeled_indices = []\nfor word, idx in lexeme2index.items():\n    try:\n        # if word in corpus\n        idx_T = word2idx[word]  # get index of word in T\n        y[idx_T] = y_l[idx]  # set values of labeled word\n        labeled_indices.append(idx_T)\n    except KeyError:\n        continue\n\n# turn multi-labels into prob distribution\ny = normalize(y, axis=1, norm='l1', copy=False)\n\nlabeled_indices.sort()\nl = labeled_indices\nu = np.setdiff1d(np.asarray(list(word2idx.values()), dtype='int32'), l)\n\nnew_order = np.append(u, l)\nt[:, :] = t[new_order][:, new_order]\ny[:] = y[new_order]\n\nT_uu = t[:len(u), :len(u)]\nT_ul = t[:len(u), len(u):]\nY_l = y[len(u):]\n\ni2i = np.zeros_like(new_order, dtype='int32')\nfor new, old in enumerate(new_order):\n    i2i[old] = new\n\nword2idx = {w: i2i[old] for w, old in word2idx.items()}\n\nprint('Tensorflow.')\nsess = tf.Session()\n\nwith tf.variable_scope(\"model\", reuse=False):\n    model = Model(len(l), len(u), NUM_EMOTIONS)\n\nsess.run(tf.global_variables_initializer())\n\nY = sess.run([model.y],\n             {model.t_uu: T_uu, model.t_ul: T_ul, model.y_l: Y_l, model.w: -0.00109, model.b: 0.90099})\n\nwith open('y_1_5000-1000-3.txt', 'w', encoding='utf-8') as f:\n    for w, i in word2idx.items():\n        print(w, str(y[i]).replace('\\n   ', '   ').replace('[', '').replace(']', ''), file=f)\n\nsess.close()\n",
			"file": "new/lp_1.py",
			"file_size": 5528,
			"file_write_time": 131440697500000000,
			"settings":
			{
				"buffer_size": 5528,
				"line_ending": "Unix"
			}
		},
		{
			"file": "bilstm_semeval.py",
			"settings":
			{
				"buffer_size": 6120,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "semeval_classifier.py",
			"settings":
			{
				"buffer_size": 5510,
				"line_ending": "Unix"
			}
		},
		{
			"file": "bilstm_classifier.py",
			"settings":
			{
				"buffer_size": 4156,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "bilstm_emo2vec.py",
			"settings":
			{
				"buffer_size": 3308,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "Anaconda Python Builder",
	"build_system_choices":
	[
		[
			[
				[
					"Anaconda Python Builder",
					""
				],
				[
					"Packages/Python/Python.sublime-build",
					""
				],
				[
					"Packages/Python/Python.sublime-build",
					"Syntax Check"
				]
			],
			[
				"Anaconda Python Builder",
				""
			]
		],
		[
			[
				[
					"Anaconda Python Builder",
					""
				],
				[
					"Packages/Python/Python.sublime-build",
					""
				],
				[
					"Packages/Python/Python.sublime-build",
					"Syntax Check"
				],
				[
					"Packages/SublimeREPL/sublimerepl_build_system_hack.sublime-build",
					""
				]
			],
			[
				"Anaconda Python Builder",
				""
			]
		],
		[
			[
				[
					"Packages/LaTeXing/LaTeX.sublime-build",
					""
				],
				[
					"Packages/LaTeXing/LaTeX.sublime-build",
					"Clean up Files"
				],
				[
					"Packages/SublimeREPL/sublimerepl_build_system_hack.sublime-build",
					""
				]
			],
			[
				"Packages/LaTeXing/LaTeX.sublime-build",
				""
			]
		]
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 93.0,
		"last_filter": "remove",
		"selected_items":
		[
			[
				"remove",
				"Package Control: Remove Package"
			],
			[
				"insat",
				"Package Control: Install Package"
			],
			[
				"sublimere",
				"SublimeREPL: Python"
			],
			[
				"install",
				"Package Control: Install Package"
			]
		],
		"width": 498.0
	},
	"console":
	{
		"height": 126.0,
		"history":
		[
			"import urllib.request,os,hashlib; h = 'df21e130d211cfc94d9b0905775a7c0f' + '1e3d39e33b79698005270310898eea76'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)"
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"expanded_folders":
	[
		"/Users/mario/GitHub/emoenv/emo2vec",
		"/Users/mario/GitHub/emoenv/emo2vec/.git",
		"/Users/mario/GitHub/emoenv/emo2vec/resources",
		"/Users/mario/GitHub/emoenv/emo2vec/resources/data"
	],
	"file_history":
	[
		"/Users/mario/GitHub/emoenv/emo2vec/lp_1.py",
		"/Users/mario/GitHub/emoenv/emo2vec/count_classifier.py",
		"/Users/mario/GitHub/emoenv/emo2vec/classifier_exp_lex.py",
		"/Users/mario/GitHub/emoenv/emo2vec/bidir_lstm_emo2vec.py",
		"/Users/mario/GitHub/emoenv/emo2vec/baseline_classifiers.py",
		"/Users/mario/GitHub/emoenv/emo2vec/resources/data/emolex.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/new/tensor_lp_300.py",
		"/Users/mario/GitHub/emoenv/emo2vec/new/lp_300.py",
		"/Users/mario/GitHub/emoenv/emo2vec/new/tensor_lp_1.py",
		"/Users/mario/GitHub/emoenv/emo2vec/new/lp_1.py",
		"/Users/mario/GitHub/emoenv/emo2vec/new/crossval_1.py",
		"/Users/mario/GitHub/emoenv/emo2vec/experiments/antsyn_lstm_emo2vec.py",
		"/Users/mario/GitHub/emoenv/emo2vec/experiments/sent_lstm_emo2vec.py",
		"/Users/mario/GitHub/emoenv/emo2vec/resources/data/twitter_sentiment/twitter_sent_corpus_25k.csv",
		"/Users/mario/GitHub/emoenv/emo2vec/semeval_classifier.py",
		"/Users/mario/GitHub/emoenv/emo2vec/bidir_lstm_semeval.py",
		"/Users/mario/GitHub/emoenv/emo2vec/lp_crossval_300.py",
		"/Users/mario/GitHub/emoenv/emo2vec/new/classifier_exp_lex.py",
		"/Users/mario/GitHub/emoenv/emo2vec/lp_crossval_1.py",
		"/Users/mario/GitHub/emoenv/emo2vec/new/crossval_300.py",
		"/Users/mario/GitHub/emoenv/emo2vec/log_sigmas_30_06.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/crossval_baselines.py",
		"/Users/mario/GitHub/emoenv/emo2vec/agreement.py",
		"/Users/mario/GitHub/emoenv/emo2vec/bilstm_classifier.py",
		"/Users/mario/GitHub/emoenv/emo2vec/new/log_sigmas_5000-5-100.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/tensor_lp_1.py",
		"/Users/mario/GitHub/emoenv/emo2vec/crossval_300.py",
		"/Users/mario/GitHub/emoenv/emo2vec/tensor_lp_300.py",
		"/Users/mario/GitHub/emoenv/emo2vec/log_1sigma_btc_2000-1000-5.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/tensor_lp_1_btc.py",
		"/Users/mario/GitHub/emoenv/emo2vec/log_1sigma_2000-1000-5.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/crossval_1.py",
		"/Users/mario/GitHub/emoenv/emo2vec/lp_300.py",
		"/Users/mario/GitHub/emoenv/emo2vec/log_1sigma_.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/batch_lp.py",
		"/Users/mario/GitHub/emoenv/emo2vec/log_1sigma_30_06.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/tensor_lp.py",
		"/Users/mario/GitHub/emoenv/emo2vec/y_lp_1sigma.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/sigmas.py",
		"/Users/mario/GitHub/emoenv/emo2vec/y_1_.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/lp.py",
		"/Users/mario/GitHub/emoenv/emo2vec/y_300_.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/log_sigmas2.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/y_lp_300sigmas_soft.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/y_lp_sigmas.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/y_sigmas.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/tensor_lp_1sigma.py",
		"/Users/mario/GitHub/emoenv/emo2vec/pmi.py",
		"/Users/mario/GitHub/emoenv/emo2vec/untitled.py",
		"/Users/mario/GitHub/emoenv/emo2vec/hashtag_corpus.py",
		"/Users/mario/GitHub/emoenv/emo2vec/pmi-classifier.py",
		"/Users/mario/GitHub/emoenv/emo2vec/model.py",
		"/Users/mario/GitHub/emoenv/emo2vec/labelprop.py",
		"/Users/mario/GitHub/emoenv/emo2vec/feedforward.py",
		"/Users/mario/GitHub/emoenv/emo2vec/simple_classifier.py",
		"/Users/mario/GitHub/emoenv/emo2vec/survey.py",
		"/Users/mario/GitHub/emoenv/emo2vec/similarity.py",
		"/Users/mario/GitHub/emoenv/emo2vec/resources/bilstm_300d.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/resources/data/hashtag/hashtag_corpus.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/statistics.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/crossval-sigma01",
		"/Users/mario/GitHub/emoenv/emo2vec/log_sigmas_5000-5-100.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/output_sigmas.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/output_1sigma.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/y_1sigma.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/twitter_stream.py",
		"/Users/mario/GitHub/emoenv/emo2vec/update_model.py",
		"/Users/mario/GitHub/emoenv/emo2vec/resources/pretrained/encow14ax-300-mincount-100-window-5-cbow.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/experiments/antsyn_lstm_doubleobj_emo2vec.py",
		"/Users/mario/GitHub/emoenv/emo2vec/twokenizer.py",
		"/Users/mario/GitHub/emoenv/emo2vec/statistics.py",
		"/Users/mario/GitHub/emoenv/emo2vec/lp_crossval.py",
		"/Users/mario/GitHub/emoenv/lib/python3.5/encodings/ascii.py",
		"/Users/mario/GitHub/emoenv/emo2vec/crossval.py",
		"/Users/mario/GitHub/emoenv/emo2vec/log_sigmas.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/resources/data/semeval/headlines.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/resources/emotion_specific/bilstm_300d.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/log_1sigma.txt",
		"/Users/mario/GitHub/emoenv/emo2vec/word2vec.py",
		"/Users/mario/GitHub/emoenv/emo2vec/param_learning.py",
		"/Users/mario/GitHub/emoenv/emo2vec/Propagation.py",
		"/Users/mario/GitHub/emoenv/emo2vec/tensor_lp_stream.py",
		"/Users/mario/GitHub/emoenv/emo2vec/expand_lexicon.py",
		"/Users/mario/Documents/Uni/SS17/thesis/latex/thesis.tex",
		"/Users/mario/GitHub/emoenv/lib/python3.5/site-packages/tensorflow/python/ops/linalg_grad.py",
		"/Users/mario/GitHub/emoenv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py",
		"/Users/mario/GitHub/emoenv/lib/python3.5/site-packages/tensorflow/python/ops/linalg_ops.py",
		"/Users/mario/GitHub/emoenv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py",
		"/Users/mario/GitHub/emoenv/lib/python3.5/site-packages/tensorflow/python/client/session.py",
		"/Users/mario/GitHub/emoenv/lib/python3.5/site-packages/tensorflow/python/ops/gen_linalg_ops.py",
		"/Users/mario/GitHub/emoenv/emo2vec/equation_solver.py",
		"/Users/mario/GitHub/emoenv/emo2vec/emo2vec.sublime-project",
		"/Users/mario/GitHub/emoenv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py",
		"/Users/mario/Library/Application Support/Sublime Text 3/Packages/Anaconda/Anaconda.sublime-settings",
		"/Users/mario/Library/Application Support/Sublime Text 3/Packages/User/Python.sublime-settings",
		"/Users/mario/GitHub/emoenv/lib/python3.5/site-packages/sklearn/preprocessing/data.py",
		"/Users/mario/GitHub/emoenv/emo2vec/resources/emotion_specific/hashtag_encow14ax-300-mincount-150-window-5-cbow.txt",
		"/Users/mario/Library/Application Support/Sublime Text 3/Packages/Anaconda/Default (OSX).sublime-keymap",
		"/Users/mario/GitHub/emoenv/lib/python3.5/site-packages/numpy/core/fromnumeric.py",
		"/Users/mario/Library/Application Support/Sublime Text 3/Packages/SideBarEnhancements/Side Bar.sublime-settings",
		"/Users/mario/Library/Application Support/Sublime Text 3/Packages/User/Default (OSX).sublime-keymap"
	],
	"find":
	{
		"height": 38.0
	},
	"find_in_files":
	{
		"height": 95.0,
		"where_history":
		[
		]
	},
	"find_state":
	{
		"case_sensitive": true,
		"find_history":
		[
			"Activation",
			"NUM_EMOTIONS",
			"VALID_SIZE",
			"TRAIN_SIZE",
			"EPOCHS",
			"EMBEDDING_DIM",
			"BATCH_SIZE",
			"LEXICON_PATH",
			"PRETRAINED_MODEL",
			"NUM_EMOTIONS",
			"TRAIN_OVER_TEST",
			"EPOCHS",
			"EMBEDDING_DIM",
			"BATCH_SIZE",
			"PRETRAINED_MODEL",
			"LABELS_PATH",
			"CORPUS_PATH",
			"Activation",
			"classification_report",
			"NUM_EMOTIONS",
			"VALID_SIZE",
			"TRAIN_SIZE",
			"EPOCHS",
			"epochs",
			"EPOCHS",
			"epochs",
			"EPOCHS",
			"EMBEDDING_DIM",
			"BATCH_SIZE",
			"batch_size",
			"BATCH_SIZE",
			"PRETRAINED_MODEL",
			"OUTPUT_MODEL",
			"activation",
			"Activation",
			"precision_recall_fscore_support",
			"NUM_EMOTIONS",
			"great ",
			"great",
			"MAX_SEQUENCE_LENGTH",
			"NUM_SAMPLES",
			"LABELS_PATH",
			"CORPUS_PATH",
			"normalize",
			"i2i",
			"idx_translator",
			"print",
			"Y_l",
			"word2index",
			"word2index_T",
			"_T",
			"partition",
			"lex_arr",
			"lexicon",
			"word2idx_T",
			"lemma2index",
			"rand",
			"lemma2index",
			"word2index_T",
			"lemma2index",
			"word2idx_T",
			"lemma2index",
			"float16",
			"32",
			"float32",
			"y_test",
			"NUM_EMOTIONS",
			"embed_dim",
			"t",
			"T",
			"t",
			"T",
			"t",
			"T",
			"t",
			"T",
			"t",
			"T",
			"t",
			"T",
			"t",
			"DataFrame",
			"sys",
			"enum",
			"sigma",
			"0\\.",
			"o\\.",
			"wtf",
			"float16",
			"random",
			"sigma",
			"sigmas",
			"sigma ",
			"cosine_dist",
			"_sum_of_squares",
			"w2i",
			"transl",
			"crossval",
			"print",
			"L",
			"lemma2index",
			"sigma",
			"sigmas",
			"wtf ",
			"lol",
			"lol ",
			":(",
			"Print",
			"def",
			"u",
			"U",
			"u",
			"U",
			"vocab",
			"0\\.",
			"[0-9]+0\\.",
			"html",
			"&",
			"tokenize",
			"EMBEDDING_DIM+NUM_EMOTIONS",
			"}\n",
			"y_test",
			"NDIMS",
			"NUM_EMOTIONS",
			"epochs",
			"terrif",
			"sad",
			"saddened"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
			"n_classes",
			"valid_size",
			"train_size",
			"epochs",
			"embed_dim",
			"batch_size",
			"lexicon_path",
			"pretrained_model",
			"n_classes",
			"train_over_test",
			"epochs",
			"embed_dim",
			"batch_size",
			"pretrained_model",
			"labels_path",
			"corpus_path",
			"n_classes",
			"train_size",
			"epochs",
			"word2idx",
			"lexeme2index",
			"float32",
			"float16",
			" 0.",
			"float32",
			" 0.",
			"EMBEDDING_DIM",
			"float32",
			"float16",
			"float32",
			"float16",
			""
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 4,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "new/classifier_exp_lex.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 4523,
						"regions":
						{
						},
						"selection":
						[
							[
								2201,
								1546
							]
						],
						"settings":
						{
							"auto_complete_triggers":
							[
								{
									"characters": ".",
									"selector": "source.python - string - comment - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								}
							],
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 11,
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "new/tensor_lp_300.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 7951,
						"regions":
						{
						},
						"selection":
						[
							[
								6865,
								6866
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 3905.0,
						"zoom_level": 1.0
					},
					"stack_index": 10,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "new/crossval_1.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6833,
						"regions":
						{
						},
						"selection":
						[
							[
								294,
								294
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 7,
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "lp_300.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 9102,
						"regions":
						{
						},
						"selection":
						[
							[
								6670,
								2980
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2231.0,
						"zoom_level": 1.0
					},
					"stack_index": 6,
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "crossval_300.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 10272,
						"regions":
						{
						},
						"selection":
						[
							[
								6895,
								6895
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "tensor_lp_300.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 7951,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 3415.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 6,
					"file": "tensor_lp_1.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6212,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2658.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				},
				{
					"buffer": 7,
					"file": "new/lp_1.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5528,
						"regions":
						{
						},
						"selection":
						[
							[
								5320,
								5320
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1805.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"type": "text"
				},
				{
					"buffer": 8,
					"file": "bilstm_semeval.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6120,
						"regions":
						{
						},
						"selection":
						[
							[
								5071,
								5071
							]
						],
						"settings":
						{
							"auto_complete_triggers":
							[
								{
									"characters": ".",
									"selector": "source.python - string - comment - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								}
							],
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 8,
					"type": "text"
				},
				{
					"buffer": 9,
					"file": "semeval_classifier.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5510,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2409.0,
						"zoom_level": 1.0
					},
					"stack_index": 9,
					"type": "text"
				},
				{
					"buffer": 10,
					"file": "bilstm_classifier.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 4156,
						"regions":
						{
						},
						"selection":
						[
							[
								2484,
								2484
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1552.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 11,
					"file": "bilstm_emo2vec.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3308,
						"regions":
						{
						},
						"selection":
						[
							[
								2189,
								2189
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 5,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 28.0
	},
	"input":
	{
		"height": 37.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.exec":
	{
		"height": 281.0
	},
	"output.find_results":
	{
		"height": 0.0
	},
	"pinned_build_system": "",
	"project": "emo2vec.sublime-project",
	"replace":
	{
		"height": 68.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"",
				"agreement.py"
			]
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 500.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"",
				"~/Documents/Uni/SS17/dl4nlp/dl4nlp_code/project/d4nlp.sublime-project"
			]
		],
		"width": 380.0
	},
	"select_symbol":
	{
		"height": 375.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 378.0
	},
	"selected_group": 0,
	"settings":
	{
	},
	"show_minimap": false,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 195.0,
	"status_bar_visible": true,
	"template_settings":
	{
	}
}
